import json
import os
import logging
import re
from collections import defaultdict
from datetime import datetime, date
from typing import Dict, List, Tuple, Optional

from dotenv import load_dotenv
from openai import OpenAI

from models import (
    AnalyzeResponse,
    ChatStats,
    ParticipantProfile,
    ParticipantStats,
    Recommendation,
    RelationshipSummary,
)
from telegram_parser import TelegramMessage, parse_telegram_html
from whatsapp_parser import parse_whatsapp_txt

load_dotenv()
logger = logging.getLogger(__name__)

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL") or "https://api.openai.com/v1",
)
MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π LLM –∏–ª–∏ –∑–∞–≥–ª—É—à–∫—É
USE_LLM = os.getenv("USE_LLM", "0") == "1"

# –õ–∏–º–∏—Ç—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
LLM_MAX_CHARS = int(os.getenv("LLM_MAX_CHARS", "60000"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "900"))

SYSTEM_PROMPT = """
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥-–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–µ—Ä–µ–ø–∏—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—á–µ–Ω—å –ø–æ–Ω—è—Ç–Ω–æ –¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞.
–°—Ç–∞—Ä–∞–π—Å—è –¥–µ–ª–∞—Ç—å –ü–û–î–†–û–ë–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã–≤–æ–¥—ã.
–†–∞–±–æ—Ç–∞–µ—à—å —Ç–æ–ª—å–∫–æ —Å –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

–í—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º –¥–≤–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞: USER_1 –∏ USER_2.
–ú–∞—Å—Å–∏–≤—ã "participants", "relationship" –∏ "recommendations" –ù–ï –î–û–õ–ñ–ï–ù–´ –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏.
–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–π –∑–∞–ø–∏—Å–∏ —Ö–æ—Ç—è –±—ã –¥–ª—è USER_1 –∏ USER_2, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî
–≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ —è–≤–Ω–æ –æ—Ç–º–µ—á–∞–π, —á—Ç–æ –≤—ã–≤–æ–¥—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.

–û—Ç–≤–µ—á–∞–π –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–∫—Ç–∞, –≤—Å–µ –ø–æ–ª—è –≤ JSON –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ó–ê–ü–û–õ–ù–ï–ù–´):

{
  "participants": [
    {
      "id": "USER_1",
      "display_name": "USER_1",
      "traits": {
        "extroversion": "–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è",
        "emotional_stability": "...",
        "other": "...",
      },
      "summary": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –∂–∏–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º –¥–ª—è –æ–±—ã–≤–∞—Ç–µ–ª—è, 20-40 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –æ–ø–∏—Å–∞–Ω–∏—è —Å—Ç–∏–ª—è –æ–±—â–µ–Ω–∏—è"
    }
  ],
  "relationship": {
    "description": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –≤–∑–∞–∏–º–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π –Ω–µ –º–µ–Ω–µ–µ 12 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
    "red_flags": ["..."],
    "green_flags": ["..."]
  },
  "recommendations": [
    { "title": "–∫—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", "text": "1-2 –∞–±–∑–∞—Ü–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Å–æ–≤–µ—Ç–æ–º" }
  ]
}
"""

# ---- –®—É–º / —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ----

MEDIA_PLACEHOLDERS = {
    "not included, change data exporting settings to download.",
    "[media omitted]",
    "<media omitted>",
    "‚Äéimage omitted",
    "‚Äéimage omitted.",
    "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ",
    "–º–µ–¥–∏–∞—Ñ–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
    "–ë–µ–∑ –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤",
    "–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ–∑–≤–æ–Ω–æ–∫",
}

SYSTEM_PATTERNS = [
    r"—Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∑–≤–æ–Ω–∫–∏ .* –∑–∞—â–∏—â–µ–Ω—ã —Å–∫–≤–æ–∑–Ω—ã–º —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ–º",
    r"–≤—ã —Å–æ–∑–¥–∞–ª–∏ –≥—Ä—É–ø–ø—É",
    r"–≤—ã –∏–∑–º–µ–Ω–∏–ª–∏ —Ñ–æ—Ç–æ –≥—Ä—É–ø–ø—ã",
    r"–≤—ã –∏–∑–º–µ–Ω–∏–ª–∏ —Ç–µ–º—É –±–µ—Å–µ–¥—ã",
    r"–≤—ã –∑–∞–∫—Ä–µ–ø–∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ",
    r"–≤—ã —É–¥–∞–ª–∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ",
    r"–ë–µ–∑ –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤",
    r"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ–∑–≤–æ–Ω–æ–∫",
]

_system_regexes = [re.compile(pat, re.IGNORECASE) for pat in SYSTEM_PATTERNS]


def _is_noise_text(text: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç "—à—É–º–æ–º":
    - –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –º–µ–¥–∏–∞
    - —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å–æ–∑–¥–∞–Ω–∏–µ/–∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø—ã –∏ —Ç.–ø.)

    –í–ê–ñ–ù–û: emoji, –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏, ¬´üëç¬ª, ¬´–æ–∫¬ª –∏ —Ç.–ø. ‚Äî –ù–ï —Å—á–∏—Ç–∞–µ–º —à—É–º–æ–º.
    """
    if not text:
        return True

    stripped = text.strip()
    if not stripped:
        return True

    low = stripped.lower()

    # –∑–∞–≥–ª—É—à–∫–∏ –º–µ–¥–∏–∞
    if low in MEDIA_PLACEHOLDERS:
        return True

    # —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ —Ä–µ–≥—É–ª—è—Ä–∫–∞–º)
    for rx in _system_regexes:
        if rx.search(low):
            return True

    return False


def _filter_noise_messages(messages: List[TelegramMessage]) -> List[TelegramMessage]:
    """
    –£–±–∏—Ä–∞–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –º—É—Å–æ—Ä:
    - —á–∏—Å—Ç—ã–µ media-–∑–∞–≥–ª—É—à–∫–∏
    - —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    –≠–º–æ–¥–∑–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –ù–ï —Ç—Ä–æ–≥–∞–µ–º.
    """
    before = len(messages)
    cleaned: List[TelegramMessage] = []

    for msg in messages:
        txt = msg.text or ""
        if _is_noise_text(txt):
            continue
        cleaned.append(msg)

    removed = before - len(cleaned)
    logger.info(
        "[noise_filter] before=%d, after=%d, removed=%d",
        before,
        len(cleaned),
        removed,
    )
    return cleaned


def _extract_json_block(content: str) -> str:
    """–ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –≤—ã—Ä–µ–∑–∞–µ–º JSON-–æ–±—ä–µ–∫—Ç { ... } –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    if not content:
        return content
    start = content.find("{")
    end = content.rfind("}")
    if start != -1 and end != -1 and end > start:
        return content[start : end + 1]
    return content


def _compute_stats_from_messages(messages: List[TelegramMessage]) -> ChatStats:
    total = len(messages)

    per_user_length: Dict[str, List[int]] = defaultdict(list)
    dates: List[datetime] = []

    for msg in messages:
        per_user_length[msg.from_name].append(len(msg.text))
        if msg.date:
            dates.append(msg.date)

    participants_stats: List[ParticipantStats] = []
    for user, lengths in per_user_length.items():
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–∞—Å—Ç–æ—è—â–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤–∏–¥–∞ USER_1, USER_2, ...
        if not user.startswith("USER_"):
            continue

        count = len(lengths)
        avg_len = sum(lengths) / count if count else 0
        participants_stats.append(
            ParticipantStats(
                id=user,
                messages_count=count,
                avg_message_length=round(avg_len, 1),
            )
        )

    participants_stats.sort(key=lambda p: p.messages_count, reverse=True)

    first_dt = min(dates) if dates else None
    last_dt = max(dates) if dates else None

    return ChatStats(
        total_messages=total,
        participants=participants_stats,
        first_message_at=first_dt,
        last_message_at=last_dt,
    )


def _compute_stats_from_plain_text(text: str) -> ChatStats:
    """–ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞."""
    lines = [ln for ln in text.splitlines() if ln.strip()]
    return ChatStats(
        total_messages=len(lines),
        participants=[],
        first_message_at=None,
        last_message_at=None,
    )


def _filter_messages_by_date(
    messages: List[TelegramMessage],
    from_date: Optional[date],
    to_date: Optional[date],
) -> List[TelegramMessage]:
    """
    –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –¥–∞—Ç (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).
    –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫.
    """
    if not from_date and not to_date:
        return messages

    filtered: List[TelegramMessage] = []
    for msg in messages:
        if not msg.date:
            continue
        d = msg.date.date()
        if from_date and d < from_date:
            continue
        if to_date and d > to_date:
            continue
        filtered.append(msg)

    if not filtered:
        logger.warning(
            "–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å, "
            "–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π.",
        )
        return messages

    return filtered


def _build_conversation_snippet(
    messages: List[TelegramMessage],
    max_chars: int = None,
    allowed_ids: Optional[set[str]] = None,
) -> str:
    """
    –ì–æ—Ç–æ–≤–∏–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è LLM: "USER_1: —Å–æ–æ–±—â–µ–Ω–∏–µ".
    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã allowed_ids ‚Äî –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è —ç—Ç–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.
    """
    if max_chars is None:
        max_chars = LLM_MAX_CHARS
    lines: List[str] = []
    total_len = 0

    for msg in messages:
        if not msg.text.strip():
            continue

        if allowed_ids is not None and msg.from_name not in allowed_ids:
            continue

        line = f"{msg.from_name}: {msg.text}"
        if total_len + len(line) > max_chars:
            break

        lines.append(line)
        total_len += len(line)

    # fallback, –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ allowed_ids –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–±—Ä–∞–ª–∏
    if not lines:
        total_len = 0
        for msg in messages:
            if not msg.text.strip():
                continue
            line = f"{msg.from_name}: {msg.text}"
            if total_len + len(line) > max_chars:
                break
            lines.append(line)
            total_len += len(line)

    return "\n".join(lines)


def _build_plain_snippet(text: str, max_chars: int = None) -> str:
    if max_chars is None:
        max_chars = LLM_MAX_CHARS
    lines = [ln for ln in text.splitlines() if ln.strip()]
    snippet = "\n".join(lines)
    return snippet[:max_chars]


def _call_llm(
    conversation_snippet: str,
) -> Tuple[List[ParticipantProfile], RelationshipSummary, List[Recommendation]]:
    user_prompt = (
        "–ù–∏–∂–µ ‚Äî –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–µ—Ä–µ–ø–∏—Å–∫–∞ (–¥–∏–∞–ª–æ–≥ –º–µ–∂–¥—É 2 —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ USER_1 –∏ USER_2).\n"
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∏–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∞–∫—Ü–∏–π:\n"
        "1) –°–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç –∫–∞–∂–¥–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞.\n"
        "2) –û–ø–∏—Å–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π.\n"
        "3) –î–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –æ–±—â–µ–Ω–∏—è.\n\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–µ–ø–∏—Å–∫–∏, –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Å–≤–µ—Ä—Ö –Ω–∞–±–ª—é–¥–∞–µ–º–æ–≥–æ.\n\n"
        "–ü–ï–†–ï–ü–ò–°–ö–ê:\n"
        + conversation_snippet
    )

    try:
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.4,
            max_tokens=LLM_MAX_TOKENS,
            response_format={"type": "json_object"},
        )
    except Exception as exc:
        logger.exception("OpenAI API call failed: %r", exc)
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: {exc}") from exc

    content = completion.choices[0].message.content
    logger.info("LLM raw content: %r", content)
    logger.info("completion.usage: %r", getattr(completion, "usage", None))

    if not content or not content.strip():
        raise RuntimeError("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")

    cleaned = _extract_json_block(content)

    try:
        data = json.loads(cleaned)
    except json.JSONDecodeError as exc:
        logger.exception("JSON decode error from LLM, cleaned=%r", cleaned)
        raise RuntimeError(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {exc}") from exc

    participants: List[ParticipantProfile] = []
    for p in data.get("participants", []):
        participants.append(
            ParticipantProfile(
                id=p.get("id") or p.get("display_name") or "USER",
                display_name=p.get("display_name") or p.get("id") or "USER",
                traits=p.get("traits", {}),
                summary=p.get("summary", ""),
            )
        )

    if not participants:
        logger.error("LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ participants, data=%r", data)
        raise RuntimeError("LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ participants")

    rel_raw = data.get("relationship", {}) or {}
    relationship = RelationshipSummary(
        description=rel_raw.get("description", ""),
        red_flags=rel_raw.get("red_flags", []) or [],
        green_flags=rel_raw.get("green_flags", []) or [],
    )

    recommendations: List[Recommendation] = []
    for r in data.get("recommendations", []):
        recommendations.append(
            Recommendation(
                title=r.get("title", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"),
                text=r.get("text", ""),
            )
        )

    return participants, relationship, recommendations


def _build_dummy_response() -> Tuple[List[ParticipantProfile], RelationshipSummary, List[Recommendation]]:
    """–ó–∞–≥–ª—É—à–∫–∞ –Ω–∞ –≤—Ä–µ–º—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏."""
    dummy_participants = [
        ParticipantProfile(
            id="USER_1",
            display_name="USER_1",
            traits={
                "extroversion": "—Å—Ä–µ–¥–Ω—è—è",
                "emotional_stability": "—Å—Ä–µ–¥–Ω—è—è",
                "agreeableness": "–≤—ã—Å–æ–∫–∞—è",
            },
            summary="–°–ø–æ–∫–æ–π–Ω—ã–π, –≤ —Ü–µ–ª–æ–º –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫, —Å–∫–ª–æ–Ω–µ–Ω —Å–≥–ª–∞–∂–∏–≤–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã.",
        ),
        ParticipantProfile(
            id="USER_2",
            display_name="USER_2",
            traits={
                "extroversion": "–≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–π",
                "emotional_stability": "–ø–æ–Ω–∏–∂–µ–Ω–Ω–∞—è",
                "assertiveness": "–≤—ã—Å–æ–∫–∞—è",
            },
            summary="–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–Ω—ã–π, –∏–Ω–æ–≥–¥–∞ –º–æ–∂–µ—Ç –¥–∞–≤–∏—Ç—å –Ω–∞ —Å–≤–æ—ë–º –º–Ω–µ–Ω–∏–∏.",
        ),
    ]

    dummy_relationship = RelationshipSummary(
        description="–û—Ç–Ω–æ—à–µ–Ω–∏—è –≤ —Ü–µ–ª–æ–º —Ç—ë–ø–ª—ã–µ, –Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —ç–ø–∏–∑–æ–¥—ã –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏–∑-–∑–∞ —Ä–∞–∑–Ω–∏—Ü—ã –≤ —Å—Ç–∏–ª–µ –æ–±—â–µ–Ω–∏—è.",
        red_flags=[
            "–ò–Ω–æ–≥–¥–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–≥–æ –∏–∑ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.",
            "–ï—Å—Ç—å —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è –∫ –ø–∞—Å—Å–∏–≤–Ω–æ–π –∞–≥—Ä–µ—Å—Å–∏–∏ –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ.",
        ],
        green_flags=[
            "–ï—Å—Ç—å —é–º–æ—Ä –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞.",
            "–û–±–µ —Å—Ç–æ—Ä–æ–Ω—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –∫ –æ–±—â–µ–Ω–∏—é –ø–æ—Å–ª–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Å–≤—è–∑–∏.",
        ],
    )

    dummy_recommendations = [
        Recommendation(
            title="–ü—Ä–æ–≥–æ–≤–∞—Ä–∏–≤–∞—Ç—å –æ–∂–∏–¥–∞–Ω–∏—è",
            text="–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å, —á–µ–≥–æ –≤—ã –æ–∂–∏–¥–∞–µ—Ç–µ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞, –≤–º–µ—Å—Ç–æ –ø–∞—Å—Å–∏–≤–Ω—ã—Ö –Ω–∞–º—ë–∫–æ–≤.",
        ),
        Recommendation(
            title="–§–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã",
            text="–°–ª–æ–∂–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –ª—É—á—à–µ –≤—ã–Ω–æ—Å–∏—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥ –∏–ª–∏ –≥–æ–ª–æ—Å, –∞ –Ω–µ —Ä–µ—à–∞—Ç—å –∏—Ö –ø–æ–∑–¥–Ω–æ –Ω–æ—á—å—é –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ.",
        ),
        Recommendation(
            title="–ë–æ–ª—å—à–µ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è",
            text="–ó–∞–º–µ—á–∞–π—Ç–µ –∏ –ø—Ä–æ–≥–æ–≤–∞—Ä–∏–≤–∞–π—Ç–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—Ä–∞–≤–∏—Ç—Å—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞ ‚Äì —ç—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –æ–±—â–∏–π —Ñ–æ–Ω –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è.",
        ),
    ]

    return dummy_participants, dummy_relationship, dummy_recommendations


def compute_chat_stats_only(chat_text: str) -> ChatStats:
    """
    –õ—ë–≥–∫–∏–π –ø–æ–¥—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–µ–∑ LLM.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ /chat_meta.
    –ó–¥–µ—Å—å —Ç–æ–∂–µ —Ä–µ–∂–µ–º —à—É–º, —á—Ç–æ–±—ã —Ü–∏—Ñ—Ä—ã –±—ã–ª–∏ –ø—Ä–æ ¬´–∂–∏–≤—ã–µ¬ª —Å–æ–æ–±—â–µ–Ω–∏—è.
    """
    is_html = "<html" in chat_text[:500].lower()

    if is_html:
        messages = parse_telegram_html(chat_text)
        if messages:
            cleaned = _filter_noise_messages(messages)
            return _compute_stats_from_messages(cleaned)
        else:
            return _compute_stats_from_plain_text(chat_text)
    else:
        wa_messages = parse_whatsapp_txt(chat_text)
        if wa_messages:
            cleaned = _filter_noise_messages(wa_messages)
            return _compute_stats_from_messages(cleaned)
        else:
            return _compute_stats_from_plain_text(chat_text)


def analyze_chat_text(
    chat_text: str,
    from_date: Optional[date] = None,
    to_date: Optional[date] = None,
) -> AnalyzeResponse:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞.
    1) –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç (Telegram HTML / WhatsApp txt / –ø—Ä–æ—á–µ–µ)
    2) –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –¥–∞—Ç–µ
    3) –†–µ–∂–µ—Ç —à—É–º (—Å–µ—Ä–≤–∏—Å–Ω—ã–µ/–º–µ–¥–∏–∞-–∑–∞–≥–ª—É—à–∫–∏), –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–æ–¥–∑–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏
    4) –°—á–∏—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    5) –õ–∏–±–æ –¥–µ—Ä–≥–∞–µ—Ç LLM, –ª–∏–±–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥–ª—É—à–∫—É
    """
    is_html = "<html" in chat_text[:500].lower()

    if is_html:
        messages = parse_telegram_html(chat_text)
        if messages:
            # —Å–Ω–∞—á–∞–ª–∞ –ø–æ –¥–∞—Ç–µ
            by_date = _filter_messages_by_date(messages, from_date, to_date)
            # –∑–∞—Ç–µ–º –≤—ã—á–∏—â–∞–µ–º —à—É–º
            filtered = _filter_noise_messages(by_date)

            stats = _compute_stats_from_messages(filtered)
            main_ids = {p.id for p in stats.participants} or None
            snippet = _build_conversation_snippet(
                filtered,
                allowed_ids=main_ids,
            )
        else:
            stats = _compute_stats_from_plain_text(chat_text)
            snippet = _build_plain_snippet(chat_text)
    else:
        wa_messages = parse_whatsapp_txt(chat_text)
        if wa_messages:
            by_date = _filter_messages_by_date(wa_messages, from_date, to_date)
            filtered = _filter_noise_messages(by_date)
            stats = _compute_stats_from_messages(filtered)
            main_ids = {p.id for p in stats.participants} or None
            snippet = _build_conversation_snippet(
                filtered,
                allowed_ids=main_ids,
            )
        else:
            stats = _compute_stats_from_plain_text(chat_text)
            snippet = _build_plain_snippet(chat_text)

    print(
        f"[analyze_chat_text] format={'html' if is_html else 'txt'}, "
        f"snippet_len={len(snippet)}, total_messages={stats.total_messages}"
    )

    logger.info(
        "[analyze_chat_text] format=%s, snippet_len=%d, total_messages=%d",
        "html" if is_html else "txt",
        len(snippet),
        stats.total_messages,
    )

    if USE_LLM:
        try:
            participants, relationship, recommendations = _call_llm(snippet)
        except Exception:
            logger.exception("LLM call failed, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            participants, relationship, recommendations = _build_dummy_response()
    else:
        participants, relationship, recommendations = _build_dummy_response()

    return AnalyzeResponse(
        participants=participants,
        relationship=relationship,
        recommendations=recommendations,
        stats=stats,
    )
